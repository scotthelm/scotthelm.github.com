<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Command Line JSON with JQ | Scott Helm</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Command Line JSON with JQ" />
<meta name="author" content="Scott Helm" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I started my first foray in to bash scripting. I have client information stored in a Mongo database. I have canonical data in Postgresql. I have indexed data in Elasticsearch. I want a quick way to check on the health of the indexes for clients both interactively and through script calls from something like Monit. I also want it to work really quickly. It would be relatively simple for me to fire up a ruby script and do what I want to do. However, the parts of our applications that deal with Mongo and Elasticsearch are found in Rails engines. For what I want, that was too heavy. I really wanted to keep this utility as light as possible, and put my fledgeling Linux skills to the test. So I dug into the Mongo CLI and discovered that I could output the results of a query back to the shell by invoking mongo with a script thusly: mongo hostname/databasename path/to/script my script returned the name and elastic search url for each client: cursor = db[&quot;clients&quot;].find({},{name: 1, elastic_search_url: 1}); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); } And this was where I ran into my first roadblock. JSON. It just so happens that JSON is not that easy to parse with awk, sed, and grep. So I lurked on stackoverflow and stumbled on the JQ library. It has been a godsend for dealing with JSON on the command line. Installation of the binary was really quick and painless: wget http://stedolan.github.io/jq/download/linux64/jq chmod +x jq sudo mv jq /usr/bin/ And I had JSON parsing available on my command line. (There are source install options as well to be found on the dowloads page). I found that I needed to modify the output of my mongo script so that it was close to valid json: cursor = db[&quot;clients&quot;].find({},{key: 1, elastic_search_url: 1}); print(&quot;{ \&quot;clients\&quot; : [&quot;); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); print(&quot;,&quot;); } print(&quot;{}&quot;) print(&quot;]}&quot;); then I turned to my trusty pal, awk and wrote this little script to clean up the client output: #! /usr/bin/awk -f /[{,\]]/ { gsub(&quot;ObjectId\(&quot;,&quot;&quot;,$0); gsub(&quot;\)&quot;,&quot;&quot;,$0); print $0 } That allowed me to pipe the valid json output to jq: mongo $MONGODB ./clients.js | ./client_cleanup.awk | jq &#39;.[&quot;clients&quot;]&#39; and I get really pretty output like this: [ { &quot;key&quot;: &quot;development_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50a6a3d86224051db2000015&quot; }, { &quot;key&quot;: &quot;bptest_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50d0cd506224052eb7000002&quot; }, { &quot;key&quot;: &quot;global_test_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50ed86586224053e54000001&quot; }, {} ] Then I started dealing with elasticsearch. I have many indexes for various clients so when I make a curl to the cluster status endpoint, the output is easily over 300 rows of console output. By contrast, the same endpoint, picking out only the name of the indexes (which is all I really want for this particular function) is amazingly simple: curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; and produces what I really want to see: [ &quot;client_509aada36224051fec000001&quot;, &quot;client_50a6a3d86224051db2000016&quot;, &quot;client_50d093c96224051f91000002&quot;, &quot;client_50d0bbb06224052add000002&quot;, &quot;client_50d0cd506224052eb7000002&quot;, &quot;client_50d1d2fa6224052c64000002&quot;, &quot;client_50d485b6622405261a000002&quot;, &quot;client_512b8556622405614f000002&quot;, &quot;client_51924d266224055568000004&quot;, &quot;client_51bf5685622405297a000002&quot;, &quot;client_51e441a8622405799900000e&quot;, ] and another call tells me how many indexes I have: $ curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; | jq &#39;length&#39; $ 11 This makes it incredibly easy to work with data retrieved from JSON services using curl. Which makes it incredibly easy to script service interruption detection and health monitoring. I am now in the process of wrapping a good many elasticsearch and mongo calls in my little bash script. I’ll be hosting the reult on github. as soon as I get it in any sort of presentable shape. However, I can’t say enough good things about the JQ library. If you need to do anything with JSON on the command line, I have found this to be the best option. Cheers, Stedolan, for the great contribution!" />
<meta property="og:description" content="I started my first foray in to bash scripting. I have client information stored in a Mongo database. I have canonical data in Postgresql. I have indexed data in Elasticsearch. I want a quick way to check on the health of the indexes for clients both interactively and through script calls from something like Monit. I also want it to work really quickly. It would be relatively simple for me to fire up a ruby script and do what I want to do. However, the parts of our applications that deal with Mongo and Elasticsearch are found in Rails engines. For what I want, that was too heavy. I really wanted to keep this utility as light as possible, and put my fledgeling Linux skills to the test. So I dug into the Mongo CLI and discovered that I could output the results of a query back to the shell by invoking mongo with a script thusly: mongo hostname/databasename path/to/script my script returned the name and elastic search url for each client: cursor = db[&quot;clients&quot;].find({},{name: 1, elastic_search_url: 1}); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); } And this was where I ran into my first roadblock. JSON. It just so happens that JSON is not that easy to parse with awk, sed, and grep. So I lurked on stackoverflow and stumbled on the JQ library. It has been a godsend for dealing with JSON on the command line. Installation of the binary was really quick and painless: wget http://stedolan.github.io/jq/download/linux64/jq chmod +x jq sudo mv jq /usr/bin/ And I had JSON parsing available on my command line. (There are source install options as well to be found on the dowloads page). I found that I needed to modify the output of my mongo script so that it was close to valid json: cursor = db[&quot;clients&quot;].find({},{key: 1, elastic_search_url: 1}); print(&quot;{ \&quot;clients\&quot; : [&quot;); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); print(&quot;,&quot;); } print(&quot;{}&quot;) print(&quot;]}&quot;); then I turned to my trusty pal, awk and wrote this little script to clean up the client output: #! /usr/bin/awk -f /[{,\]]/ { gsub(&quot;ObjectId\(&quot;,&quot;&quot;,$0); gsub(&quot;\)&quot;,&quot;&quot;,$0); print $0 } That allowed me to pipe the valid json output to jq: mongo $MONGODB ./clients.js | ./client_cleanup.awk | jq &#39;.[&quot;clients&quot;]&#39; and I get really pretty output like this: [ { &quot;key&quot;: &quot;development_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50a6a3d86224051db2000015&quot; }, { &quot;key&quot;: &quot;bptest_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50d0cd506224052eb7000002&quot; }, { &quot;key&quot;: &quot;global_test_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50ed86586224053e54000001&quot; }, {} ] Then I started dealing with elasticsearch. I have many indexes for various clients so when I make a curl to the cluster status endpoint, the output is easily over 300 rows of console output. By contrast, the same endpoint, picking out only the name of the indexes (which is all I really want for this particular function) is amazingly simple: curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; and produces what I really want to see: [ &quot;client_509aada36224051fec000001&quot;, &quot;client_50a6a3d86224051db2000016&quot;, &quot;client_50d093c96224051f91000002&quot;, &quot;client_50d0bbb06224052add000002&quot;, &quot;client_50d0cd506224052eb7000002&quot;, &quot;client_50d1d2fa6224052c64000002&quot;, &quot;client_50d485b6622405261a000002&quot;, &quot;client_512b8556622405614f000002&quot;, &quot;client_51924d266224055568000004&quot;, &quot;client_51bf5685622405297a000002&quot;, &quot;client_51e441a8622405799900000e&quot;, ] and another call tells me how many indexes I have: $ curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; | jq &#39;length&#39; $ 11 This makes it incredibly easy to work with data retrieved from JSON services using curl. Which makes it incredibly easy to script service interruption detection and health monitoring. I am now in the process of wrapping a good many elasticsearch and mongo calls in my little bash script. I’ll be hosting the reult on github. as soon as I get it in any sort of presentable shape. However, I can’t say enough good things about the JQ library. If you need to do anything with JSON on the command line, I have found this to be the best option. Cheers, Stedolan, for the great contribution!" />
<link rel="canonical" href="/2013/09/28/command-line-json-with-jq.html" />
<meta property="og:url" content="/2013/09/28/command-line-json-with-jq.html" />
<meta property="og:site_name" content="Scott Helm" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2013-09-28T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Command Line JSON with JQ" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Scott Helm"},"dateModified":"2013-09-28T00:00:00-04:00","datePublished":"2013-09-28T00:00:00-04:00","description":"I started my first foray in to bash scripting. I have client information stored in a Mongo database. I have canonical data in Postgresql. I have indexed data in Elasticsearch. I want a quick way to check on the health of the indexes for clients both interactively and through script calls from something like Monit. I also want it to work really quickly. It would be relatively simple for me to fire up a ruby script and do what I want to do. However, the parts of our applications that deal with Mongo and Elasticsearch are found in Rails engines. For what I want, that was too heavy. I really wanted to keep this utility as light as possible, and put my fledgeling Linux skills to the test. So I dug into the Mongo CLI and discovered that I could output the results of a query back to the shell by invoking mongo with a script thusly: mongo hostname/databasename path/to/script my script returned the name and elastic search url for each client: cursor = db[&quot;clients&quot;].find({},{name: 1, elastic_search_url: 1}); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); } And this was where I ran into my first roadblock. JSON. It just so happens that JSON is not that easy to parse with awk, sed, and grep. So I lurked on stackoverflow and stumbled on the JQ library. It has been a godsend for dealing with JSON on the command line. Installation of the binary was really quick and painless: wget http://stedolan.github.io/jq/download/linux64/jq chmod +x jq sudo mv jq /usr/bin/ And I had JSON parsing available on my command line. (There are source install options as well to be found on the dowloads page). I found that I needed to modify the output of my mongo script so that it was close to valid json: cursor = db[&quot;clients&quot;].find({},{key: 1, elastic_search_url: 1}); print(&quot;{ \\&quot;clients\\&quot; : [&quot;); while ( cursor.hasNext() ) { printjsononeline( cursor.next() ); print(&quot;,&quot;); } print(&quot;{}&quot;) print(&quot;]}&quot;); then I turned to my trusty pal, awk and wrote this little script to clean up the client output: #! /usr/bin/awk -f /[{,\\]]/ { gsub(&quot;ObjectId\\(&quot;,&quot;&quot;,$0); gsub(&quot;\\)&quot;,&quot;&quot;,$0); print $0 } That allowed me to pipe the valid json output to jq: mongo $MONGODB ./clients.js | ./client_cleanup.awk | jq &#39;.[&quot;clients&quot;]&#39; and I get really pretty output like this: [ { &quot;key&quot;: &quot;development_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50a6a3d86224051db2000015&quot; }, { &quot;key&quot;: &quot;bptest_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50d0cd506224052eb7000002&quot; }, { &quot;key&quot;: &quot;global_test_client&quot;, &quot;elastic_search_url&quot;: &quot;http://search.dev:9200&quot;, &quot;_id&quot;: &quot;50ed86586224053e54000001&quot; }, {} ] Then I started dealing with elasticsearch. I have many indexes for various clients so when I make a curl to the cluster status endpoint, the output is easily over 300 rows of console output. By contrast, the same endpoint, picking out only the name of the indexes (which is all I really want for this particular function) is amazingly simple: curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; and produces what I really want to see: [ &quot;client_509aada36224051fec000001&quot;, &quot;client_50a6a3d86224051db2000016&quot;, &quot;client_50d093c96224051f91000002&quot;, &quot;client_50d0bbb06224052add000002&quot;, &quot;client_50d0cd506224052eb7000002&quot;, &quot;client_50d1d2fa6224052c64000002&quot;, &quot;client_50d485b6622405261a000002&quot;, &quot;client_512b8556622405614f000002&quot;, &quot;client_51924d266224055568000004&quot;, &quot;client_51bf5685622405297a000002&quot;, &quot;client_51e441a8622405799900000e&quot;, ] and another call tells me how many indexes I have: $ curl -s -XGET &#39;http://search.dev:9200/_status&#39; | jq &#39;.[&quot;indices&quot;]&#39; | jq &#39;keys&#39; | jq &#39;length&#39; $ 11 This makes it incredibly easy to work with data retrieved from JSON services using curl. Which makes it incredibly easy to script service interruption detection and health monitoring. I am now in the process of wrapping a good many elasticsearch and mongo calls in my little bash script. I’ll be hosting the reult on github. as soon as I get it in any sort of presentable shape. However, I can’t say enough good things about the JQ library. If you need to do anything with JSON on the command line, I have found this to be the best option. Cheers, Stedolan, for the great contribution!","headline":"Command Line JSON with JQ","mainEntityOfPage":{"@type":"WebPage","@id":"/2013/09/28/command-line-json-with-jq.html"},"url":"/2013/09/28/command-line-json-with-jq.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/rss.rss" title="Scott Helm" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-49576057-1"></script>
<script>
  window['ga-disable-UA-49576057-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-49576057-1');
</script>

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest"></head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Scott Helm</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/all_posts.html">All Posts</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Command Line JSON with JQ</h1>
    <p class="post-meta"><time class="dt-published" datetime="2013-09-28T00:00:00-04:00" itemprop="datePublished">
        2013-09-28
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I started my first foray in to bash scripting. I have client information stored
in a Mongo database. I have canonical data in Postgresql. I have indexed data
in Elasticsearch. I want a quick way to check on the health of the indexes for
clients both interactively and through script calls from something like Monit.
I also want it to work really quickly.</p>

<p>It would be relatively simple for me to fire up a ruby script and do what I want to do.
However, the parts of our applications that deal with Mongo and Elasticsearch are
found in Rails engines. For what I want, that was too heavy. I really wanted to
keep this utility as light as possible, and put my fledgeling Linux skills to the test.</p>

<p>So I dug into the <a href="http://docs.mongodb.org/manual/administration/scripting/">Mongo CLI</a>
and discovered that I could output the results of a query back to the shell by
invoking mongo with a script thusly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mongo hostname/databasename path/to/script
</code></pre></div></div>

<p>my script returned the name and elastic search url for each client:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cursor = db["clients"].find({},{name: 1, elastic_search_url: 1});
while ( cursor.hasNext() ) {
   printjsononeline( cursor.next() );
}
</code></pre></div></div>

<p>And this was where I ran into my first roadblock. JSON. It just so happens that
JSON is not that easy to parse with <code class="language-plaintext highlighter-rouge">awk</code>, <code class="language-plaintext highlighter-rouge">sed</code>, and <code class="language-plaintext highlighter-rouge">grep</code>. So I lurked on stackoverflow
and stumbled on the <a href="http://stedolan.github.io/jq/">JQ</a> library. It has been a
godsend for dealing with JSON on the command line.</p>

<p>Installation of the binary was really quick and painless:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget http://stedolan.github.io/jq/download/linux64/jq
chmod +x jq
sudo mv jq /usr/bin/
</code></pre></div></div>

<p>And I had JSON parsing available on my command line. (There are source install
options as well to be found on the <a href="http://stedolan.github.io/jq/download/">dowloads page</a>).</p>

<p>I found that I needed to modify the output of my mongo script so that it was
close to valid json:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cursor = db["clients"].find({},{key: 1, elastic_search_url: 1});
print("{ \"clients\" : [");
while ( cursor.hasNext() ) {
   printjsononeline( cursor.next() );
   print(",");
}
print("{}")
print("]}");
</code></pre></div></div>

<p>then I turned to my trusty pal, <code class="language-plaintext highlighter-rouge">awk</code> and wrote this little script to clean up
the client output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#! /usr/bin/awk -f
/[{,\]]/ {
  gsub("ObjectId\(","",$0);
  gsub("\)","",$0);
  print $0
}
</code></pre></div></div>

<p>That allowed me to pipe the valid json output to <code class="language-plaintext highlighter-rouge">jq</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  mongo $MONGODB ./clients.js | ./client_cleanup.awk | jq '.["clients"]'
</code></pre></div></div>

<p>and I get really pretty output like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
  {
    "key": "development_client",
    "elastic_search_url": "http://search.dev:9200",
    "_id": "50a6a3d86224051db2000015"
  },
  {
    "key": "bptest_client",
    "elastic_search_url": "http://search.dev:9200",
    "_id": "50d0cd506224052eb7000002"
  },
  {
    "key": "global_test_client",
    "elastic_search_url": "http://search.dev:9200",
    "_id": "50ed86586224053e54000001"
  },
  {}
]
</code></pre></div></div>

<p>Then I started dealing with elasticsearch. I have many indexes for various clients
so when I make a curl to the cluster status endpoint, the output is easily over 
300 rows of console output. By contrast, the same endpoint, picking out only the
name of the indexes (which is all I really want for this particular function) is
amazingly simple:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -s -XGET 'http://search.dev:9200/_status' | jq '.["indices"]' | jq 'keys'
</code></pre></div></div>

<p>and produces what I really want to see:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
  "client_509aada36224051fec000001",
  "client_50a6a3d86224051db2000016",
  "client_50d093c96224051f91000002",
  "client_50d0bbb06224052add000002",
  "client_50d0cd506224052eb7000002",
  "client_50d1d2fa6224052c64000002",
  "client_50d485b6622405261a000002",
  "client_512b8556622405614f000002",
  "client_51924d266224055568000004",
  "client_51bf5685622405297a000002",
  "client_51e441a8622405799900000e",
]
</code></pre></div></div>

<p>and another call tells me how many indexes I have:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ curl -s -XGET 'http://search.dev:9200/_status' | jq '.["indices"]' | jq 'keys' | jq 'length'
$ 11
</code></pre></div></div>

<p>This makes it incredibly easy to work with data retrieved from JSON services
using <code class="language-plaintext highlighter-rouge">curl</code>. Which makes it incredibly easy to script service interruption
detection and health monitoring.</p>

<p>I am now in the process of wrapping a good many elasticsearch and mongo calls in
my little bash script. I’ll be hosting the reult on github. as soon as I get it
in any sort of presentable shape. However, I can’t say enough good things about
the JQ library. If you need to do anything with JSON on the command line, I
have found this to be the best option. Cheers, Stedolan, for the great contribution!</p>


  </div><div id="disqus_thread"></div>
  <!--
 <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'scotthelm'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function () {
   var s = document.createElement('script'); s.async = true;
   s.type = 'text/javascript';
   s.src = '//' + disqus_shortname + '.disqus.com/count.js';
   (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
   }());
 </script>
  -->
 
  <!--

  -->
  <script>
    var disqus_shortname = 'scotthelm'; // required: replace example with your forum shortname
    var disqus_config = function () {
      this.page.url = '/2013/09/28/command-line-json-with-jq.html';
      this.page.identifier = '/2013/09/28/command-line-json-with-jq.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://scotthelm.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><a class="u-url" href="/2013/09/28/command-line-json-with-jq.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/rss.rss">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">&copy; Scott Helm 2023</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>Father, Husband, Technologist, Actor, Musician, Life-long Learner</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/scotthelm" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://linkedin.com/in/scott-helm-4a1aa52" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/1518852/plasticide" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://twitter.com/scott_helm" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
